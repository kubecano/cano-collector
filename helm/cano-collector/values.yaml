# Default values for cano-collector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

fullnameOverride: ""

global:
  clusterDomain: "cluster.local"

# Install the kube-prometheus-stack chart with Kubecano
enablePrometheusStack: false
enabledManagedConfiguration: false
enableServiceMonitors: true
monitorHelmReleases: true

isSmallCluster: false

automountServiceAccountToken: true

image:
  registry: ghcr.io/kubecano

destinations:
  # Slack destinations configuration
  # Each destination must have exactly one of: api_key or api_key_value_from
  slack: 
    # - name: "prod-slack"
    #   # Option 1: Direct API key (for development/testing)
    #   api_key: "xoxb-your-slack-bot-token"
    #   
    #   # Option 2: API key from external Kubernetes secret
    #   api_key_value_from:
    #     secretName: "kubecano-slack-api-keys"
    #     secretKey: "prod-slack"
    #   
    #   slack_channel: "#prod-alerts"
    #   grouping_interval: 30
    #   unfurl_links: true
    #   
    #   # Thread Management Settings
    #   threading:
    #     enabled: true
    #     cache_ttl: "10m"              # Cache duration for thread relationships
    #     search_limit: 100             # Max messages to search in history
    #     search_window: "24h"          # Time window for history search
    #     fingerprint_in_metadata: true # Include fingerprint in message metadata
    #   
    #   # Enrichment Display Settings
    #   enrichments:
    #     format_as_blocks: true        # Use Slack blocks instead of plain text
    #     color_coding: true            # Color-code enrichments by type
    #     table_formatting: "enhanced"  # "simple", "enhanced", or "attachment"
    #     max_table_rows: 20           # Convert large tables to files
    #     attachment_threshold: 1000    # Characters threshold for file conversion

teams: [ ]

workflows:
  # Workflow configuration
  # Each workflow defines triggers and actions for alert processing
  active_workflows:
    # Default workflow for firing alerts - creates and dispatches issues
    - name: "default-firing-alerts"
      triggers:
        - on_alertmanager_alert:
            status: "firing"  # React to firing alerts only
      actions:
        - action_type: "create_issue"
          data:
            title: "{{.alert_name}}"
            aggregation_key: "{{.alert_name}}"
            description: "{{.annotations.summary}}"
            severity: "{{.severity}}"
        - action_type: "dispatch_issue"
          data: {}
      stop: false

    # Default workflow for resolved alerts - resolves and dispatches issues
    - name: "default-resolved-alerts"
      triggers:
        - on_alertmanager_alert:
            status: "resolved"  # React to resolved alerts only
      actions:
        - action_type: "resolve_issue"
          data:
            title: "[RESOLVED] {{.alert_name}}"
            aggregation_key: "{{.alert_name}}"
            description: "{{.annotations.summary}}"
            severity: "{{.severity}}"
        - action_type: "dispatch_issue"
          data: {}
      stop: false

collector:
  image:
    name: cano-collector
    imageOverride: ~
    tag: ~
    pullPolicy: IfNotPresent
  enableTelemetry: true
  sentry_dsn: https://4f1a66f025c60830fec303a094dcdf94@o1120648.ingest.sentry.io/6156573
  appEnv: production
  logLevel: info
  tracing:
    mode: "disabled" # disabled | local | remote
    endpoint: ""
  ginMode: release
  annotations: { }
  labels: { }
  customServiceAccount: ""
  imagePullSecrets: [ ]
  startupProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  resources:
    limits:
      cpu: ~
      memory: ~
      ephemeralStorage: 1Gi
    requests:
      cpu: 250m
      memory: 1024Mi
  service:
    annotations: { }
  serviceAccount:
    # image pull secrets added to the service account. Any pod using the service account will get those
    imagePullSecrets: [ ]
    annotations: { }
  serviceMonitor:
    additionalLabels: { }
    path: /metrics
    interval:
    scrapeTimeout:
    relabelings: [ ]
    metricRelabelings: [ ]
    honorLabels: false
  securityContext:
    container:
      allowPrivilegeEscalation: false
      capabilities: { }
      privileged: false
      readOnlyRootFilesystem: false
    pod: { }
  nodeSelector: { }
  tolerations: [ ]
  affinity: { }
  priorityClassName: ""
  # Label and Annotation Enrichment configuration
  enrichment:
    labels:
      enabled: true
      # Display format: "table" or "json"
      displayFormat: "table"
      # List of label keys to include (if empty, all labels are included)
      includeLabels: []
      # List of label keys to exclude (applied after includeLabels)
      excludeLabels:
        - "__name__"
        - "job"
        - "instance"
        - "__meta_kubernetes_pod_container_port_name"
        - "__meta_kubernetes_pod_container_port_number"
        - "__meta_kubernetes_pod_container_port_protocol"
        - "__meta_kubernetes_pod_ready"
        - "__meta_kubernetes_pod_phase"
        - "__meta_kubernetes_pod_ip"
        - "__meta_kubernetes_pod_host_ip"
        - "__meta_kubernetes_pod_node_name"
        - "__meta_kubernetes_pod_uid"
        - "__meta_kubernetes_namespace"
        - "__meta_kubernetes_service_port_name"
        - "__meta_kubernetes_service_port_number"
        - "__meta_kubernetes_service_port_protocol"
        - "__meta_kubernetes_service_cluster_ip"
        - "__meta_kubernetes_service_external_name"
        - "__meta_kubernetes_service_type"
        - "__meta_kubernetes_ingress_scheme"
        - "__meta_kubernetes_ingress_host"
        - "__meta_kubernetes_ingress_path"
        - "__meta_kubernetes_ingress_class_name"
    annotations:
      enabled: true
      # Display format: "table" or "json"
      displayFormat: "table"
      # List of annotation keys to include (if empty, all annotations are included)
      includeAnnotations: []
      # List of annotation keys to exclude (applied after includeAnnotations)
      excludeAnnotations:
        - "kubectl.kubernetes.io/last-applied-configuration"
        - "deployment.kubernetes.io/revision"
        - "control-plane.alpha.kubernetes.io/leader"
        - "prometheus.io/scrape"
        - "prometheus.io/port"
        - "prometheus.io/path"

kube-prometheus-stack:
  alertmanager:
    tplConfig: true
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: [ '...' ]
        group_wait: 1s
        group_interval: 1s
        repeat_interval: 4h
        receiver: 'kubecano'
        routes:
          - match:
              alertname: Watchdog
            receiver: 'null'
      receivers:
        - name: 'null'
        - name: 'kubecano'
          webhook_configs:
            - url: 'http://cano-collector.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/api/alerts'
              send_resolved: true
    alertmanagerSpec:
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          memory: 128Mi
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 10Gi
  kubeProxy:
    enabled: false
  prometheus:
    prometheusSpec:
      resources:
        requests:
          cpu: 50m
          memory: 2Gi
        limits:
          memory: 2Gi
      retention: 15d
      # we set a value slightly lower than the 100Gi below
      # the retentionSize uses the suffix GB but it is really Gi units
      # that is, the retentionSize is measured in base2 units just like Gi, Mi, etc
      retentionSize: "99GB"

      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 100Gi
  prometheus-node-exporter:
    service:
      port: 9104
      targetPort: 9104
    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        memory: 50Mi
    # disable node-exporter on fargate because fargate doesn't allow daemonsets
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                    - fargate
  prometheusOperator:
    resources:
      requests:
        cpu: 100m
    prometheusConfigReloader:
      resources:
        limits:
          cpu: 0
  kube-state-metrics:
    resources:
      requests:
        cpu: 10m
        memory: 256Mi
      limits:
        memory: 256Mi
