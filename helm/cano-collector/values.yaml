# Default values for cano-collector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

fullnameOverride: ""

global:
  clusterDomain: "cluster.local"

# Install the kube-prometheus-stack chart with Kubecano
enablePrometheusStack: false
enabledManagedConfiguration: false
enableServiceMonitors: true
monitorHelmReleases: true

isSmallCluster: false

automountServiceAccountToken: true

image:
  registry: ghcr.io/kubecano

destinations:
  # Slack destinations configuration
  # Each destination must have exactly one of: api_key or api_key_value_from
  slack: 
    # - name: "prod-slack"
    #   # Option 1: Direct API key (for development/testing)
    #   api_key: "xoxb-your-slack-bot-token"
    #   
    #   # Option 2: API key from external Kubernetes secret
    #   api_key_value_from:
    #     secretName: "kubecano-slack-api-keys"
    #     secretKey: "prod-slack"
    #   
    #   slack_channel: "#prod-alerts"
    #   grouping_interval: 30
    #   unfurl_links: true
  msteams: [ ]

teams: [ ]

collector:
  image:
    name: cano-collector
    imageOverride: ~
    tag: ~
    pullPolicy: IfNotPresent
  enableTelemetry: true
  sentry_dsn: https://4f1a66f025c60830fec303a094dcdf94@o1120648.ingest.sentry.io/6156573
  appEnv: production
  logLevel: info
  tracing:
    mode: "disabled" # disabled | local | remote
    endpoint: ""
  ginMode: release
  annotations: { }
  labels: { }
  customServiceAccount: ""
  imagePullSecrets: [ ]
  startupProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  readinessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1
  resources:
    limits:
      cpu: ~
      memory: ~
      ephemeralStorage: 1Gi
    requests:
      cpu: 250m
      memory: 1024Mi
  service:
    annotations: { }
  serviceAccount:
    # image pull secrets added to the service account. Any pod using the service account will get those
    imagePullSecrets: [ ]
    annotations: { }
  serviceMonitor:
    additionalLabels: { }
    path: /metrics
    interval:
    scrapeTimeout:
    relabelings: [ ]
    metricRelabelings: [ ]
    honorLabels: false
  securityContext:
    container:
      allowPrivilegeEscalation: false
      capabilities: { }
      privileged: false
      readOnlyRootFilesystem: false
    pod: { }
  nodeSelector: { }
  tolerations: [ ]
  affinity: { }
  priorityClassName: ""

kube-prometheus-stack:
  alertmanager:
    tplConfig: true
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: [ '...' ]
        group_wait: 1s
        group_interval: 1s
        repeat_interval: 4h
        receiver: 'kubecano'
        routes:
          - match:
              alertname: Watchdog
            receiver: 'null'
      receivers:
        - name: 'null'
        - name: 'kubecano'
          webhook_configs:
            - url: 'http://cano-collector.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}/api/alerts'
              send_resolved: true
    alertmanagerSpec:
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          memory: 128Mi
      storage:
        volumeClaimTemplate:
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 10Gi
  kubeProxy:
    enabled: false
  prometheus:
    prometheusSpec:
      resources:
        requests:
          cpu: 50m
          memory: 2Gi
        limits:
          memory: 2Gi
      retention: 15d
      # we set a value slightly lower than the 100Gi below
      # the retentionSize uses the suffix GB but it is really Gi units
      # that is, the retentionSize is measured in base2 units just like Gi, Mi, etc
      retentionSize: "99GB"

      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: [ "ReadWriteOnce" ]
            resources:
              requests:
                storage: 100Gi
  prometheus-node-exporter:
    service:
      port: 9104
      targetPort: 9104
    resources:
      requests:
        cpu: 50m
        memory: 50Mi
      limits:
        memory: 50Mi
    # disable node-exporter on fargate because fargate doesn't allow daemonsets
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: eks.amazonaws.com/compute-type
                  operator: NotIn
                  values:
                    - fargate
  prometheusOperator:
    resources:
      requests:
        cpu: 100m
    prometheusConfigReloader:
      resources:
        limits:
          cpu: 0
  kube-state-metrics:
    resources:
      requests:
        cpu: 10m
        memory: 256Mi
      limits:
        memory: 256Mi
