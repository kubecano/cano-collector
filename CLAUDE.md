# Cano-Collector Project Guide

## Project Overview

Cano-collector is a Kubernetes event monitoring and alert processing system. 

**Current Implementation**: Processes alerts from Alertmanager via webhook
**Future Implementation**: Will also process direct Kubernetes events

The system processes notifications through configurable workflows and sends them to various destinations (Slack, MS Teams, etc.).

## Architecture

- **Go-based microservice** deployed in Kubernetes (namespace: `kubecano`)
- **Dual processing architecture**:
  - **Current**: Alert-driven processing via Alertmanager webhooks
  - **Future**: Direct Kubernetes event watching (planned)
- **Multi-destination support** for alerts and notifications
- **Configurable workflows** for different alert/event types and resources
- **Helm chart deployment** with kube-prometheus-stack integration

## Processing Flows

### Current: Alert Processing Flow
1. **Kubernetes Resources** (pods, deployments, etc.) generate states/metrics
2. **kube-state-metrics** exports resource states as Prometheus metrics
3. **Prometheus** scrapes metrics and evaluates alert rules
4. **Alertmanager** receives firing alerts and routes them to cano-collector
5. **Cano-collector** processes alerts via `/api/alerts` webhook endpoint:
   - **Alert Handler** automatically converts alerts to issues
   - **Workflow Engine** executes actions to enrich issues
   - **Alert Dispatcher** automatically sends enriched issues to destinations
6. **Destinations** (Slack, etc.) receive enriched notifications

### Future: Event Processing Flow (Planned)
1. **Kubernetes Events** (BackOff, CrashLoopBackOff, etc.) generated by cluster
2. **Cano-collector** watches events via Kubernetes API
3. **Event Handler** processes raw Kubernetes events
4. **Workflow Engine** executes configured actions based on event types
5. **Destinations** receive real-time event notifications

## Key Directories

- `pkg/` - Main application code organized by domain
- `config/` - Configuration management (destinations, teams, workflows)
- `helm/` - Helm chart for Kubernetes deployment
- `docs/` - Comprehensive documentation (Sphinx-based)
- `tests/` - Test pod suite for validation scenarios
- `dashboards/` - Grafana dashboards for monitoring

## Development Commands

### Testing
```bash
# Run all tests
make test

# Run tests with coverage
go test -cover ./...

# Specific package tests
go test ./pkg/alert/...
```

### Building
```bash
# Build binary
go build -o cano-collector main.go

# Build Docker image
docker build -t cano-collector .

# Build test image
./build_test_image.sh
```

### Deployment
```bash
# Deploy with Helm
helm install cano-collector ./helm/cano-collector

# Upgrade deployment
helm upgrade cano-collector ./helm/cano-collector

# Check deployment status
kubectl get pods -n kubecano -l app=cano-collector
```

## Testing with Test Pod Suite

The project includes a comprehensive test suite in `/tests/` for validating cano-collector functionality:

**Current Testing**: Validates Prometheus alert processing from Kubernetes resource failures
**Future Testing**: Will also validate direct Kubernetes event processing

### Understanding Test Flow (Current: Alert-based)
1. **Test pods** are deployed to generate specific failure states
2. **kube-state-metrics** exports pod states as Prometheus metrics
3. **Prometheus** evaluates alert rules and fires alerts (e.g., `KubePodCrashLooping`)
4. **Alertmanager** routes alerts to cano-collector via webhook
5. **Cano-collector** processes alerts and sends notifications to configured destinations

### Future Test Flow (Event-based - Planned)
1. **Test pods** are deployed to generate specific failure states
2. **Kubernetes API** generates events (BackOff, CrashLoopBackOff, etc.)
3. **Cano-collector** watches and receives events directly
4. **Event processing** triggers workflows and sends notifications immediately

### Quick Start
```bash
# Validate cano-collector is running
./tests/scripts/validate-collector.sh

# Run basic crash test (creates CrashLoopBackOff pod)
./tests/scripts/run-test.sh crash-loop busybox-crash

# Monitor Kubernetes events (for debugging)
kubectl get events -n test-pods --sort-by=.metadata.creationTimestamp -w

# Check if alerts are fired in Prometheus
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
# Visit http://localhost:9090/alerts

# Check cano-collector logs for alert processing
kubectl logs -n kubecano -l app=cano-collector -f

# Cleanup test resources
./tests/scripts/cleanup.sh
```

### Validation Commands
```bash
# Check kube-state-metrics sees the test pod
kubectl port-forward -n monitoring svc/kube-prometheus-stack-kube-state-metrics 8080:8080
curl -s "http://localhost:8080/metrics" | grep -i "busybox-crash"

# Query Prometheus for pod metrics
curl -s "http://localhost:9090/api/v1/query?query=kube_pod_container_status_restarts_total" | jq '.data.result[] | select(.metric.pod=="busybox-crash-test")'

# Check Alertmanager for active alerts
kubectl port-forward -n monitoring svc/kube-prometheus-stack-alertmanager 9093:9093  
curl -s "http://localhost:9093/api/v2/alerts" | jq '.[] | select(.labels.pod=="busybox-crash-test")'
```

### Test Categories

Each test scenario creates specific resource states that trigger corresponding Prometheus alerts:

- **crash-loop**: CrashLoopBackOff scenarios → `KubePodCrashLooping` alerts
  - `busybox-crash`: Simple container exit → CrashLoopBackOff
  - `jdk-crash`: Java exception crash
  - `nginx-crash`: Configuration validation failure
- **oom**: Out of Memory tests → `KubePodOOMKilled` alerts  
  - `memory-bomb`: Quick memory exhaustion
  - `gradual-oom`: Slow memory leak
- **image-pull**: ImagePullBackOff scenarios → `KubePodImagePullBackOff` alerts
  - `nonexistent-image`: 404 image not found
  - `private-registry`: Authentication failure
- **resource-limits**: Resource constraint tests → `KubePodPending` alerts
  - `cpu-starved`: Insufficient CPU requests
  - `impossible-resources`: Unrealistic resource demands
- **network**: Network failure tests → Application-specific alerts
- **deployments**: Deployment failures → `KubeDeploymentReplicasMismatch` alerts
- **jobs**: Job failures → `KubeJobFailed` alerts
- **services**: Service issues → Endpoint-related alerts

### Expected Alert Names
Based on kube-prometheus-stack default rules:
- `KubePodCrashLooping` - Pod in CrashLoopBackOff state
- `KubePodNotReady` - Pod not ready for extended period
- `KubePodImagePullBackOff` - Image pull failures
- `KubeJobFailed` - Job completion failures
- `KubeDeploymentReplicasMismatch` - Deployment scaling issues

## Configuration

### Main Config Structure
```go
type Config struct {
    Teams        []TeamConfig        `yaml:"teams"`
    Destinations []DestinationConfig `yaml:"destinations"`
    Workflows    []WorkflowConfig    `yaml:"workflows"`
}
```

### Key Config Files
- `helm/cano-collector/values.yaml` - Main Helm configuration
- `config/` - Go configuration structs and validation

## Monitoring

### Health Checks
```bash
# Health endpoint
curl http://localhost:8080/health

# Metrics endpoint
curl http://localhost:8080/metrics
```

### Key Metrics
- `events_processed_total` - Total events processed
- `alerts_sent_total` - Total alerts sent to destinations
- `workflow_executions_total` - Workflow execution counts

## Debugging

### Common Issues
1. **RBAC Permissions**: Ensure service account can watch events
2. **Destination Configuration**: Verify webhook URLs and credentials
3. **Workflow Triggers**: Check alert labels and conditions

### Log Analysis
```bash
# View logs (cano-collector runs in kubecano namespace)
kubectl logs -n kubecano -l app=cano-collector -f

# Check alert processing
kubectl logs -n kubecano cano-collector-8545cf675-jj8tx | grep -i "alert\|firing\|slack"

# Search for specific alert processing
kubectl logs -n kubecano -l app=cano-collector | grep "KubePodCrashLooping"

# Monitor real-time alert processing
kubectl logs -n kubecano -l app=cano-collector -f | grep "Alert processed successfully"
```

### Common Alert Processing Patterns
Look for these log patterns when debugging:
```
"Alert processed successfully" - Alert received and processed
"Slack message sent successfully" - Notification delivered
"Issue sent successfully" - Alert formatted and sent to destination
"destination":"default-slack" - Target destination identified
"severity":"LOW|MEDIUM|HIGH" - Alert severity mapping
```

## Development Patterns

### Processing Flows
1. **Current (Alerts)**: **Prometheus Alert** → **Alertmanager Webhook** → **Alert Handler** → **Workflow Engine** → **Action Execution** → **Destination**
2. **Future (Events)**: **Kubernetes Event** → **Event Watcher** → **Event Handler** → **Workflow Engine** → **Action Execution** → **Destination**

### Key Interfaces
- `alert.Handler` - Processes incoming alerts (current)
- `event.Handler` - Processes Kubernetes events (planned)
- `destination.Destination` - Handles message delivery
- `workflow.Engine` - Executes workflow actions

### Adding New Features
1. **New Destination**: Implement `destination.Destination` interface
2. **New Workflow Action**: Implement `actions.Action` interface  
3. **New Alert Type**: Add to alert processing logic (current)
4. **New Event Type**: Add to event processing logic (planned)

## Project Dependencies

- **Kubernetes client-go** - Kubernetes API interactions (current: minimal, future: event watching)
- **Prometheus client** - Metrics collection
- **Gin framework** - HTTP server (webhook endpoints)
- **Logrus** - Structured logging
- **Testify** - Testing framework

## Release Process

1. **Version bump** in `VERSION` file
2. **Update CHANGELOG.md** with changes  
3. **Tag release** - triggers automated build and deployment
4. **Helm chart** automatically updated via release-please

## Documentation

- **Sphinx docs** in `/docs/` directory
- **API reference** auto-generated from code
- **Architecture docs** explain system design
- **Configuration examples** for all destinations

### Building Documentation
```bash
cd docs/
make html
# Output in _build/html/
```

## Test Infrastructure

### Test Pod Validation
The `/tests/` directory contains comprehensive test scenarios that validate cano-collector's ability to:
- Detect and process Kubernetes events
- Trigger appropriate workflows
- Send notifications to configured destinations
- Handle various failure scenarios

### Expected Alert Types
Based on kube-prometheus-stack integration, cano-collector processes:
- **Pod Alerts**: KubePodCrashLooping, KubePodImagePullBackOff, KubePodOOMKilled, KubePodNotReady
- **Deployment Alerts**: KubeDeploymentReplicasMismatch, KubeDeploymentGenerationMismatch
- **Job Alerts**: KubeJobFailed, KubeJobCompletion
- **Service Alerts**: KubeServiceDown, EndpointSliceAway

### Alert Receiver Configuration
Cano-collector is configured as `kubecano` receiver in Alertmanager:
```yaml
# Alertmanager routes alerts to cano-collector via:
receivers:
- name: "kubecano"
  webhook_configs:
  - url: 'http://cano-collector.kubecano.svc.cluster.local:8080/api/alerts'
```

## Integration Points

### Alertmanager Integration
Cano-collector receives alerts from kube-prometheus-stack Alertmanager:
```yaml
# Current webhook configuration in Alertmanager:
receivers:
- name: "kubecano"
  webhook_configs:
  - url: 'http://cano-collector.kubecano.svc.cluster.local:8080/api/alerts'
    send_resolved: true
    
# Test webhook connectivity:
kubectl port-forward -n kubecano svc/cano-collector 8080:8080
curl -X POST http://localhost:8080/api/alerts -H "Content-Type: application/json" -d '[]'
```

### Prometheus Integration
Cano-collector integrates with kube-prometheus-stack:
- **Metrics endpoint**: `/metrics` scraped by Prometheus
- **Health checks**: `/livez` and `/readyz` for Kubernetes probes  
- **Alert source**: Receives alerts generated by Prometheus rules
- **Key metrics**:
  - `http_requests_total` - HTTP endpoint usage
  - `alerts_processed_total` - Alert processing metrics
  - `notifications_sent_total` - Destination delivery metrics

```bash
# Access Prometheus metrics
kubectl port-forward -n kubecano svc/cano-collector 8080:8080
curl http://localhost:8080/metrics | grep cano_collector

# Check health endpoints
curl http://localhost:8080/livez   # Liveness probe
curl http://localhost:8080/readyz  # Readiness probe
```

## Security Considerations

- **RBAC**: Minimal required permissions (no Kubernetes event watching needed)
- **Secret Management**: Secure storage of destination credentials (Slack tokens, etc.)
- **Network Policies**: Restrict traffic to necessary services only
- **Webhook Security**: Ensure Alertmanager can reach cano-collector endpoint
- **Image Security**: Regular vulnerability scanning of container images

## Troubleshooting Guide

### Common Issues and Solutions

1. **No alerts received from Alertmanager**
   ```bash
   # Check Alertmanager configuration
   kubectl get secret -n monitoring alertmanager-kube-prometheus-stack-alertmanager -o yaml
   
   # Verify webhook connectivity
   kubectl port-forward -n kubecano svc/cano-collector 8080:8080
   curl -X POST http://localhost:8080/api/alerts -H "Content-Type: application/json" -d '[]'
   ```

2. **Test pods not generating alerts**
   ```bash
   # Verify kube-state-metrics is running
   kubectl get pods -n monitoring | grep kube-state-metrics
   
   # Check if metrics are exposed
   kubectl port-forward -n monitoring svc/kube-prometheus-stack-kube-state-metrics 8080:8080
   curl http://localhost:8080/metrics | grep kube_pod_container_status_waiting_reason
   
   # Check Prometheus alert rules
   kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
   # Visit http://localhost:9090/rules
   ```

3. **Alerts not reaching destinations (Slack)**
   ```bash
   # Check cano-collector logs for destination errors
   kubectl logs -n kubecano -l app=cano-collector | grep -i "slack\|error\|failed"
   
   # Verify destination configuration
   kubectl get configmap -n kubecano -o yaml | grep -A 10 -B 10 slack
   ```

### Debugging Alert Flow
1. **Pod State** → Check with `kubectl describe pod`
2. **Metrics** → Verify in kube-state-metrics `/metrics`  
3. **Prometheus** → Check `/alerts` endpoint
4. **Alertmanager** → Verify in `/api/v2/alerts`
5. **Cano-collector** → Check logs for processing
6. **Destination** → Verify delivery (Slack channel, etc.)

## Workflow Configuration and Message Types

### Workflow Engine Architecture

Cano-collector uses a **workflow engine** that processes alerts through configurable **action pipelines**:

1. **Triggers** define when workflows execute (alert status, severity, name)
2. **Actions** define what happens (create issue, get logs, send notifications)
3. **Action Registry** manages available action types and factories
4. **Action Executor** runs actions sequentially with enrichment support

### Available Action Types

#### Core Actions (Always Available)
- **`label_filter`** - Filters alerts by label matching rules
- **`severity_router`** - Routes alerts based on severity mapping
- **`pod_logs`** - Retrieves Kubernetes pod logs and adds as enrichment
- **`issue_enrichment`** - Adds metadata enrichments to existing Issues

#### Action Configuration Format
```yaml
actions:
  - action_type: "pod_logs"
    data:
      max_lines: 2000
      tail_lines: 300
      timestamps: true
      previous: true
      include_container: true
```

### Default Workflow Configuration

#### 1. **Basic Alert Processing** (Always Active)
```yaml
# default-firing-alerts
triggers:
  - on_alertmanager_alert:
      status: "firing"
actions:
  - action_type: "issue_enrichment"
    data:
      include_metadata: true
      include_labels: true
      custom_title: "{{.alert_name}}"
```

#### 2. **Pod Log Collection** (Conditional)
```yaml
# pod-logs-kube-crash-looping (enabled)
triggers:
  - on_alertmanager_alert:
      status: "firing"
      alert_name: "KubePodCrashLooping"
actions:
  - action_type: "pod_logs"
    data:
      previous: true        # Get previous container logs
      max_lines: 2000      # Log limit
      tail_lines: 300      # Tail limit
      timestamps: true     # Include timestamps
      include_container: true
```

### Slack Message Structure

#### **Basic Alert Message** (All alerts get this)
- **Header**: 🔥 Alert firing 🔴 High + Alert title
- **Description**: From `{{.annotations.summary}}`
- **Action Buttons**: 🔍 Investigate, 🔕 Silence, 📊 Prometheus
- **Colored Attachment**: Source, cluster, namespace, timestamp

#### **Enhanced Message with Logs** (KubePodCrashLooping + critical alerts)
- **Basic message structure** (above)
- **+ Enrichment Block**: Pod logs as file attachment
  - **Title**: `"Pod Logs: namespace/podname (container)"`
  - **File**: `pod-logs-namespace-podname-timestamp.log`
  - **Previous logs**: For crash investigation
  - **Current logs**: For context

#### **Threading and Fingerprinting**
- **Fingerprint generation**: Based on title + source + subject
- **Thread grouping**: Resolved alerts reply to firing alert threads
- **Cache management**: 10-minute TTL, 24-hour search window

### Alert Type → Message Format Mapping

| Alert Name | Workflows | Message Type | Logs | Enrichments |
|------------|-----------|--------------|------|-------------|
| `KubePodCrashLooping` | `default-firing-alerts` + `pod-logs-kube-crash-looping` | Enhanced | ✅ Previous + Current | Pod logs file |
| `KubePodImagePullBackOff` | `default-firing-alerts` | Basic | ❌ | None |
| `KubePodOOMKilled` | `default-firing-alerts` | Basic | ❌ | None |
| `PodCrashLooping` (critical) | `default-firing-alerts` + `pod-logs-critical-crashes` | Enhanced | ✅ Previous + Current | Pod logs file |
| `JavaAppError` | `default-firing-alerts` + `pod-logs-java` (disabled) | Java Enhanced | ✅ Extended (5000 lines) | Java pod logs |

### Workflow Debugging

#### Log Patterns to Look For:
```bash
# Workflow execution
kubectl logs -n kubecano -l app=cano-collector | grep "workflow\|action"

# Alert processing patterns
"Alert processed successfully"     # Alert received and processed
"Workflow matched for alert"       # Workflow trigger matched
"Action executed successfully"     # Action completed
"Enrichment added to issue"        # Pod logs attached
"Slack message sent successfully"  # Final delivery
```

#### Common Workflow Issues:
1. **No pod logs in message** → Check `pod-logs-*` workflow enabled and alert name match
2. **Workflow parsing errors** → Check `action_type`/`data` YAML structure 
3. **Missing action factories** → Verify action registry includes required types
4. **Label extraction fails** → Check alert has `pod` or `instance` labels

- `main.go` - Action factory registration and initialization

## Important Notes

- **Local Execution**: Do not run cano-collector locally - it requires files mounted at specific locations. Execution is only possible on the cluster.
- **Main Action Factory Registration**: `main.go` - Action factory registration and initialization